{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN and CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-mnist in c:\\users\\recie\\anaconda3\\lib\\site-packages (0.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNISTの読み込み\n",
    "import numpy as np\n",
    "from mnist import MNIST\n",
    "mndata = MNIST(\"./le4nn/\")\n",
    "X_train, Y_train = mndata.load_training()\n",
    "X_test, Y_test = mndata.load_testing()\n",
    "X_train = np.array(X_train)\n",
    "X_train = X_train.reshape((X_train.shape[0],1,28,28))\n",
    "Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)\n",
    "X_test = X_test.reshape((X_test.shape[0],1,28,28))\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST教師データの正規化\n",
    "def normalize_mnist(x):\n",
    "    x = x.astype(np.float32)\n",
    "    x /= 255.0\n",
    "    return x\n",
    "\n",
    "X_train = normalize_mnist(X_train)\n",
    "X_test = normalize_mnist(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シード値固定\n",
    "np.random.seed(404)\n",
    "\n",
    "# バッチサイズ\n",
    "b = 100\n",
    "\n",
    "# 入力層のノードの数\n",
    "d = 28 * 28\n",
    "\n",
    "# 中間層のノード数\n",
    "m = 50\n",
    "\n",
    "# 出力層のノードの数\n",
    "c = 10\n",
    "\n",
    "# 学習率\n",
    "eta = 0.01\n",
    "\n",
    "# エポック数\n",
    "epoch = 30\n",
    "\n",
    "# 教師データ数\n",
    "n = 60000\n",
    "\n",
    "# one-hot vector\n",
    "T_train = np.eye(c)[Y_train]\n",
    "T_test = np.eye(c)[Y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シグモイド関数\n",
    "# in : x / out : y\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, train_flag=True):\n",
    "        y = 1.0 / (1.0 + np.exp(-x))\n",
    "        self.y = y\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        dx = dy * (1.0 - self.y) * self.y\n",
    "        return dx\n",
    "\n",
    "    \n",
    "# ソフトマックス関数 & クロスエントロピー誤差\n",
    "# in : x / out : y\n",
    "class SoftmaxWithCrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        self.loss = None\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        alpha = np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x - alpha)\n",
    "        sum_exp_x = np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return exp_x / sum_exp_x\n",
    "    \n",
    "    def cross_entropy(self, y, t):\n",
    "        # log(0)対策\n",
    "        delta = 1e-7\n",
    "        t_log_y = t * np.log(y + delta)\n",
    "        return -np.sum(t_log_y) / y.shape[0]\n",
    "    \n",
    "    def forward(self, x, t, train_flag=True):\n",
    "        self.y = self.softmax(x)\n",
    "        self.t = t\n",
    "        self.loss = self.cross_entropy(self.y, self.t)\n",
    "        return self.y, self.loss\n",
    "    \n",
    "    def backward(self, dy=1):\n",
    "        dx = (self.y - self.t) / self.t.shape[0]\n",
    "        return dx\n",
    "\n",
    "    \n",
    "# 全結合層\n",
    "# in : x, W, b / out : y\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.x = None\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.x_shape_origin = None\n",
    "    \n",
    "    def forward(self, x, train_flag=True):\n",
    "        # CNNの時はxの形状が2次元じゃないのでその対策をしないといけない\n",
    "        self.x_shape_origin = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "        y = np.dot(self.x, self.W) + self.b\n",
    "        # 今回はAffineの後にConvolutionとかが来ないので形を直さなくていい\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        dx = np.dot(dy, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dy)\n",
    "        self.db = np.sum(dy, axis=0)\n",
    "        # 入力を元の形に戻す\n",
    "        dx = dx.reshape(*self.x_shape_origin)\n",
    "        return dx\n",
    "\n",
    "    \n",
    "# ReLU関数\n",
    "# in : x / out : y\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flag=True):\n",
    "        self.mask = (x <= 0)\n",
    "        y = x.copy()\n",
    "        y[self.mask] = 0\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        dy[self.mask] = 0\n",
    "        dx = dy\n",
    "        return dx\n",
    "\n",
    "    \n",
    "# Dropout\n",
    "class Dropout:\n",
    "    def __init__(self, rho=0.5):\n",
    "        self.rho = rho\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flag=True):\n",
    "        if train_flag:\n",
    "            # 入力と同じshapeの行列に範囲[0.0,1.0)の一様分布で値を埋めて、ρより大きい要素だけTrue\n",
    "            self.mask = np.random.rand(*x.shape) > self.rho\n",
    "            y = x * self.mask\n",
    "            return y\n",
    "        else:\n",
    "            return x * (1.0 - self.rho)\n",
    "\n",
    "    def backward(self, dy):\n",
    "        dx = dy * self.mask\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "# Batch Normalization\n",
    "class BatchNormalization:\n",
    "    def __init__(self, gamma=1.0, beta=0.0):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "        self.epsilon = 1e-7   \n",
    "        self.xu = None\n",
    "        self.std = None\n",
    "        self.xhat = None\n",
    "        # テスト時に使う平均と分散\n",
    "        self.test_mean = None\n",
    "        self.test_var = None\n",
    "        # 普通のNNとCNNでは入力の形が異なるので処理が微妙に変わる\n",
    "        self.input_shape = None\n",
    "     \n",
    "    def forward(self, x, train_flag=True):\n",
    "        # 入力の形は覚えておく\n",
    "        self.input_shape = x.shape\n",
    "        # CNNならば形を2次元に合わせる\n",
    "        if x.ndim != 2:\n",
    "            n, ch, h, w = x.shape\n",
    "            x = x.reshape(n, -1)\n",
    "        # 学習時とテスト時で処理が異なる\n",
    "        if train_flag:\n",
    "            mean = np.mean(x, axis=0)\n",
    "            xu = x - mean\n",
    "            self.xu = xu\n",
    "            self.mean = mean\n",
    "            var = np.mean(xu**2, axis=0)\n",
    "            std = np.sqrt(var + self.epsilon)\n",
    "            self.std = std\n",
    "            xhat = (x - mean) / std\n",
    "            self.xhat = xhat\n",
    "            y = self.gamma * xhat + self.beta\n",
    "            # 最初の1回はテスト時に使う平均と分散を初期化\n",
    "            if self.test_mean is None:\n",
    "                self.test_mean = np.zeros(x.shape[1])\n",
    "                self.test_var = np.zeros(x.shape[1])\n",
    "            # テスト時に使う平均と分散を修正\n",
    "            self.test_mean = 0.9 * self.test_mean + 0.1 * mean\n",
    "            self.test_var = 0.9 * self.test_var + 0.1 * var\n",
    "        else:\n",
    "            std = np.sqrt(self.test_var + self.epsilon)\n",
    "            a = self.gamma / std\n",
    "            b = self.beta - self.gamma * self.test_mean / std\n",
    "            y = a * x + b\n",
    "        # CNNの場合は形を戻さないといけないので\n",
    "        y = y.reshape(*self.input_shape)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        # CNNならば形を2次元に合わせる\n",
    "        if dy.ndim != 2:\n",
    "            n, ch, h, w = dout.shape\n",
    "            dy = dy.reshape(n, -1)\n",
    "        # バッチサイズ\n",
    "        b = dy.shape[1]\n",
    "        dxhat = dy * self.gamma\n",
    "        dvar = np.sum(dxhat * self.xu * (-0.5) / self.std**3, axis=0)\n",
    "        dmean = (-np.sum(dxhat / self.std, axis=0) + dvar * np.sum((-2.0) * self.xu, axis=0) / b)\n",
    "        dx = dxhat / self.std + dvar * 2.0 * self.xu / b + dmean / b\n",
    "        dgamma = np.sum(dy * self.xhat, axis=0)\n",
    "        self.dgamma = dgamma\n",
    "        dbeta = np.sum(dy, axis=0)\n",
    "        self.dbeta = dbeta\n",
    "        # CNNの場合は形を戻さないといけないので\n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, eta = 0.01):\n",
    "        self.eta = eta\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        # パラメータ更新\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.eta * grads[key]\n",
    "\n",
    "            \n",
    "class MomentumSGD:\n",
    "    def __init__(self, eta=0.01, alpha=0.9):\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.dW = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.dW is None:\n",
    "            self.dW = {}\n",
    "            for key, val in params.items():\n",
    "                self.dW[key] = np.zeros_like(val)\n",
    "        # パラメータ更新\n",
    "        for key in params.keys():\n",
    "            self.dW[key] = self.alpha * self.dW[key] - self.eta * grads[key]\n",
    "            params[key] += self.dW[key]\n",
    "\n",
    "            \n",
    "class AdaGrad:\n",
    "    def __init__(self, eta=0.001, h0=1e-8):\n",
    "        self.eta = eta\n",
    "        self.h = None\n",
    "        self.h0 = h0\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.full_like(val, self.h0)\n",
    "        # パラメータ更新\n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.eta * grads[key] / np.sqrt(self.h[key])\n",
    "\n",
    "            \n",
    "class RMSProp:\n",
    "    def __init__(self, eta=0.001, rho=0.9, epsilon=1e-8):\n",
    "        self.eta = eta\n",
    "        self.rho = rho\n",
    "        self.epsilon = epsilon\n",
    "        self.h = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        # パラメータ更新\n",
    "        for key in params.keys():\n",
    "            self.h[key] = self.rho * self.h[key] + (1.0 - self.rho) * grads[key] * grads[key]\n",
    "            params[key] -= self.eta * grads[key] / (np.sqrt(self.h[key]) + self.epsilon)\n",
    "\n",
    "            \n",
    "class AdaDelta:\n",
    "    def __init__(self, rho=0.95, epsilon=1e-6):\n",
    "        self.rho = rho\n",
    "        self.epsilon = epsilon\n",
    "        self.h = None\n",
    "        self.s = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            self.s = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                self.s[key] = np.zeros_like(val)\n",
    "        # パラメータ更新\n",
    "        for key in params.keys():\n",
    "            self.h[key] = self.rho * self.h[key] + (1.0 - self.rho) * grads[key] * grads[key]\n",
    "            dW = -(np.sqrt(self.s[key] + self.epsilon) * grads[key] / np.sqrt(self.h[key] + self.epsilon))\n",
    "            self.s[key] = self.rho * self.s[key] + (1.0 - self.rho) * dW * dW\n",
    "            params[key] += dW\n",
    "\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = {}\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        # パラメータ更新\n",
    "        for key in params.keys():\n",
    "            self.t += 1\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1.0 - self.beta1) * grads[key]\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1.0 - self.beta2) * grads[key] * grads[key]\n",
    "            mhat = self.m[key] / (1.0 - np.power(self.beta1, self.t))\n",
    "            vhat = self.v[key] / (1.0 - np.power(self.beta2, self.t))\n",
    "            params[key] -= self.alpha * mhat / (np.sqrt(vhat) + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータの初期値\n",
    "\n",
    "# 活性化関数がSigmoidの時\n",
    "def xavier(prev_size, param_shape):\n",
    "    param = np.random.randn(*param_shape) * np.sqrt(1.0 / prev_size)\n",
    "    return param\n",
    "\n",
    "# 活性化関数がReLUの時\n",
    "def he(prev_size, param_shape):\n",
    "    param = np.random.randn(*param_shape) * np.sqrt(2.0 / prev_size)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算量を減らすために入力データとフィルターを行列に展開する関数\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, padding=0):\n",
    "    input_size, input_ch, input_h, input_w = input_data.shape\n",
    "    # 出力のサイズを計算\n",
    "    output_h = (input_h + 2 * padding - filter_h) // stride + 1\n",
    "    output_w = (input_w + 2 * padding - filter_w) // stride + 1\n",
    "    # 入力にパディング処理をする\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (padding, padding), (padding, padding)], 'constant')\n",
    "    # 求めたい行列を0初期化\n",
    "    col = np.zeros((input_size, input_ch, filter_h, filter_w, output_h, output_w))  \n",
    "    # imgの展開処理\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * output_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * output_w\n",
    "            # y:y_max:stride : y～y_max-1の要素をstride個毎に取り出す\n",
    "            # x:x_max:stride : x～x_max-1の要素をstride個毎に取り出す\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "    \"\"\"\n",
    "    # 上の処理は実質これ（https://qiita.com/kurumen-b/items/236c6255959a266cefaa）\n",
    "    # これは愚直なので4重ループとなり遅い\n",
    "    for move_y in range(output_h):\n",
    "        for move_x in range(output_w):\n",
    "            for y in range(filter_h):\n",
    "                for x in range(filter_w):\n",
    "                    # move_yは高々output_h-1の値しかとらない（output_hで抑えられる）\n",
    "                    # つまりy + stride * move_yは上記のy:y_max:strideと等価である\n",
    "                    # y + stride * move_yのループはyからstride * output_h未満の範囲をstride個ごとに取るということなので\n",
    "                    col[:, :, y, x, move_y, move_x] = img[:, :, y + stride * move_y, x + stride * move_x]\n",
    "    \"\"\"\n",
    "    # colを入力データ数 * 出力の高さ * 出力の幅 * 入力のチャネル数 * フィルターの高さ * フィルターの幅に並べ替える\n",
    "    # reshapeして縦 : 入力データ数 * 出力の高さ * 出力の幅, 横 : 入力のチャネル数 * フィルターの高さ * フィルターの幅の行列に変換\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(input_size * output_h * output_w, -1)\n",
    "    return col\n",
    "\n",
    "\n",
    "# im2colの逆伝播版\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, padding=0):\n",
    "    input_size, input_ch, input_h, input_w = input_shape\n",
    "    # 出力のサイズを計算\n",
    "    output_h = (input_h + 2 * padding - filter_h) // stride + 1\n",
    "    output_w = (input_w + 2 * padding - filter_w) // stride + 1\n",
    "    # colは元々(入力データ数 * 出力の高さ * 出力の幅) * (入力のチャネル数 * フィルターの高さ * フィルターの幅)の行列(2次元)\n",
    "    # colを入力データ数 * 出力の高さ * 出力の幅 * 入力のチャネル数 * フィルターの高さ * フィルターの幅にreshape(6次元)\n",
    "    # さらに入力データ数 * 入力のチャネル数 * フィルターの高さ * フィルターの幅 * 出力の高さ * 出力の幅に並び替える\n",
    "    col = col.reshape(input_size, output_h, output_w, input_ch, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "    # 求めたい入力を0初期化\n",
    "    img = np.zeros((input_size, input_ch, input_h + 2 * padding + stride - 1, input_w + 2 * padding + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * output_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * output_w\n",
    "            # im2colの時と逆にしただけ\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "    # パディングを取っ払う\n",
    "    input_data = img[:, :, padding:input_h + padding, padding:input_w + padding]\n",
    "    return input_data\n",
    "\n",
    "\n",
    "# 畳み込み層\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, padding=0):\n",
    "        self.x = None\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "\n",
    "    def forward(self, x, train_flag=True):\n",
    "        self.x = x\n",
    "        filter_size, channel, filter_h, filter_w = self.W.shape\n",
    "        input_size, channel, input_h, input_w = x.shape\n",
    "        # 出力のサイズを計算\n",
    "        output_h = int((input_h + 2 * self.padding - filter_h) / self.stride) + 1\n",
    "        output_w = int((input_w + 2 * self.padding - filter_w) / self.stride) + 1\n",
    "        # 入力データを行列に展開（縦が入力データ数 * 出力の高さ * 出力の幅）\n",
    "        col = im2col(x, filter_h, filter_w, self.stride, self.padding)\n",
    "        self.col = col\n",
    "        # フィルターを行列に展開（横がフィルター数）\n",
    "        col_W = self.W.reshape(filter_size, -1).T\n",
    "        self.col_W = col_W\n",
    "        # 畳み込みを計算\n",
    "        y = np.dot(col, col_W) + self.b\n",
    "        # 入力データ数 * フィルター数 * 出力の高さ * 出力の幅の形に整える\n",
    "        y = y.reshape(input_size, output_h, output_w, filter_size).transpose(0, 3, 1, 2)\n",
    "        return y\n",
    "\n",
    "    def backward(self, dy):\n",
    "        filter_size, channel, filter_h, filter_w = self.W.shape\n",
    "        # dyは元々入力データ数 * フィルター数 * 出力の高さ * 出力の幅(4次元)\n",
    "        # dyを入力データ数 * 出力の高さ * 出力の幅 * フィルター数に並び替える\n",
    "        # さらに(入力データ数 * 出力の高さ * 出力の幅) * フィルター数にreshape(2次元)\n",
    "        dy = dy.transpose(0,2,3,1).reshape(-1, filter_size)\n",
    "        # Affineのbackwardとほぼ同じ\n",
    "        # dxにcol2imが、dWにreshapeが必要なところが違う\n",
    "        dcol = np.dot(dy, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, filter_h, filter_w, self.stride, self.padding)\n",
    "        self.dW = np.dot(self.col.T, dy)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(filter_size, channel, filter_h, filter_w)\n",
    "        self.db = np.sum(dy, axis=0)\n",
    "        return dx\n",
    "\n",
    "\n",
    "# プーリング層（maxプーリング）\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, padding=0):\n",
    "        self.x = None\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.max_id = None\n",
    "\n",
    "    def forward(self, x, train_flag=True):\n",
    "        self.x = x\n",
    "        input_size, channel, input_h, input_w = x.shape\n",
    "        # 出力のサイズを計算\n",
    "        output_h = int(1 + (input_h - self.pool_h) / self.stride)\n",
    "        output_w = int(1 + (input_w - self.pool_w) / self.stride)\n",
    "        # 入力データを行列に展開\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.padding)\n",
    "        # 縦が入力データ数 * 出力の高さ * 出力の幅 * チャネル数になるよう整形\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "        # 行毎の最大値を持つ要素番号を求める（backwardで使うので）\n",
    "        max_id = np.argmax(col, axis=1)\n",
    "        self.max_id = max_id\n",
    "        # 行毎の最大値を求める\n",
    "        y = np.max(col, axis=1)\n",
    "        # 入力データ数 * チャネル数 * 出力の高さ * 出力の幅の形に整える\n",
    "        y = y.reshape(input_size, output_h, output_w, channel).transpose(0, 3, 1, 2)\n",
    "        return y\n",
    "\n",
    "    def backward(self, dy):\n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        # dyを入力データ数 * 出力の高さ * 出力の幅 * チャネル数に並び替える\n",
    "        dy = dy.transpose(0, 2, 3, 1)\n",
    "        # 最大値を与えた要素のインデックスについてのみ誤差を伝播したい\n",
    "        # それ以外は0なのでまずは零行列を作る\n",
    "        dz = np.zeros((dy.size, pool_size))\n",
    "        # 最大値の要素を持っていたインデックスに対応する場所に誤差の値（dy）を埋め込む\n",
    "        dz[np.arange(self.max_id.size), self.max_id.flatten()] = dy.flatten()\n",
    "        # 形を整える\n",
    "        dz = dz.reshape(dy.shape + (pool_size,)) \n",
    "        dcol = dz.reshape(dz.shape[0] * dz.shape[1] * dz.shape[2], -1)\n",
    "        # 画像の形に戻す\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.padding)     \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample NN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class NormalNN:\n",
    "    def __init__(self, d, m, c):\n",
    "        # パラメータ初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = xavier(d, (d, m))\n",
    "        self.params['W2'] = xavier(m, (m, c))\n",
    "        self.params['b1'] = xavier(d, (1, m))\n",
    "        self.params['b2'] = xavier(m, (1, c))\n",
    "        # NN生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Sigmoid'] = Sigmoid()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithCrossEntropy()\n",
    "        \n",
    "    # 順伝播\n",
    "    def forward(self, x, t, tflag=True):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x, train_flag=tflag)\n",
    "        y, loss = self.lastLayer.forward(x, t)\n",
    "        return y, loss\n",
    "    \n",
    "    # 逆伝播\n",
    "    def backpropagation(self):\n",
    "        dy = 1.0\n",
    "        dy = self.lastLayer.backward(dy)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dy = layer.backward(dy)\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        return grads\n",
    "    \n",
    "    # ファイルへの保存\n",
    "    def save_params(self, filename):\n",
    "        np.savez(filename, W1=self.params['W1'], W2=self.params['W2'], b1=self.params['b1'], b2=self.params['b2'])\n",
    "    \n",
    "    # ファイルからの読み込み\n",
    "    def load_params(self, filename):\n",
    "        npz = np.load(filename)\n",
    "        self.params['W1'] = npz['W1']\n",
    "        self.params['W2'] = npz['W2']\n",
    "        self.params['b1'] = npz['b1']\n",
    "        self.params['b2'] = npz['b2']\n",
    "        self.layers['Affine1'].W = self.params['W1']\n",
    "        self.layers['Affine1'].b = self.params['b1']\n",
    "        self.layers['Affine2'].W = self.params['W2']\n",
    "        self.layers['Affine2'].b = self.params['b2']\n",
    "    \n",
    "    # テストデータを用いた精度確認\n",
    "    def test_acc(self, x, t):\n",
    "        ok = 0\n",
    "        ng = 0\n",
    "        for i in range(x.shape[0]):\n",
    "            xx = x[[i]].reshape((1, d))\n",
    "            tt = t[[i]]\n",
    "            y, e = self.forward(xx, tt, False)\n",
    "            res = np.argmax(y)\n",
    "            ans = np.argmax(tt)\n",
    "            if res == ans:\n",
    "                ok += 1\n",
    "            else:\n",
    "                ng += 1\n",
    "        print(\"正答数 : {}\".format(ok))\n",
    "        print(\"誤答数 : {}\".format(ng))\n",
    "        print(\"正答率 : {}%\".format(float(ok) * 100.0 / float(ok + ng)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample NN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedNN:\n",
    "    def __init__(self, d, m, c):\n",
    "        # パラメータ初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = he(d, (d, m))\n",
    "        self.params['W2'] = he(m, (m, c))\n",
    "        self.params['Gamma'] = 1.0\n",
    "        self.params['Beta'] = 0.0\n",
    "        self.params['b1'] = he(d, (1, m))\n",
    "        self.params['b2'] = he(m, (1, c))\n",
    "        # NN生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['BatchNormalization'] = BatchNormalization(self.params['Gamma'], self.params['Beta'])\n",
    "        self.layers['ReLU'] = ReLU()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithCrossEntropy()\n",
    "        \n",
    "    # 順伝播\n",
    "    def forward(self, x, t, tflag=True):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x, train_flag=tflag)\n",
    "        y, loss = self.lastLayer.forward(x, t)\n",
    "        return y, loss\n",
    "    \n",
    "    # 逆伝播\n",
    "    def backpropagation(self):\n",
    "        dy = 1.0\n",
    "        dy = self.lastLayer.backward(dy)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dy = layer.backward(dy)\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        grads['Gamma'] = self.layers['BatchNormalization'].dgamma\n",
    "        grads['Beta'] = self.layers['BatchNormalization'].dbeta\n",
    "        return grads\n",
    "    \n",
    "    # ファイルへの保存\n",
    "    def save_params(self, filename):\n",
    "        np.savez(filename, W1=self.params['W1'], W2=self.params['W2'], \n",
    "                 b1=self.params['b1'], b2=self.params['b2'], \n",
    "                 Gamma=self.params['Gamma'], Beta=self.params['Beta'],\n",
    "                 BN_T_M=self.layers['BatchNormalization'].test_mean,\n",
    "                 BN_T_V=self.layers['BatchNormalization'].test_var)\n",
    "    \n",
    "    # ファイルからの読み込み\n",
    "    def load_params(self, filename):\n",
    "        npz = np.load(filename)\n",
    "        self.params['W1'] = npz['W1']\n",
    "        self.params['W2'] = npz['W2']\n",
    "        self.params['b1'] = npz['b1']\n",
    "        self.params['b2'] = npz['b2']\n",
    "        self.params['Gamma'] = npz['Gamma']\n",
    "        self.params['Beta'] = npz['Beta']\n",
    "        bn_test_mean = npz['BN_T_M']\n",
    "        bn_test_var = npz['BN_T_V']\n",
    "        self.layers['Affine1'].W = self.params['W1']\n",
    "        self.layers['Affine1'].b = self.params['b1']\n",
    "        self.layers['Affine2'].W = self.params['W2']\n",
    "        self.layers['Affine2'].b = self.params['b2']\n",
    "        self.layers['BatchNormalization'].gamma = self.params['Gamma']\n",
    "        self.layers['BatchNormalization'].beta = self.params['Beta']\n",
    "        self.layers['BatchNormalization'].test_mean = bn_test_mean\n",
    "        self.layers['BatchNormalization'].test_var = bn_test_var\n",
    "    \n",
    "    # テストデータを用いた精度確認\n",
    "    def test_acc(self, x, t):\n",
    "        ok = 0\n",
    "        ng = 0\n",
    "        for i in range(x.shape[0]):\n",
    "            xx = x[[i]].reshape((1, d))\n",
    "            tt = t[[i]]\n",
    "            y, e = self.forward(xx, tt, False)\n",
    "            res = np.argmax(y)\n",
    "            ans = np.argmax(tt)\n",
    "            if res == ans:\n",
    "                ok += 1\n",
    "            else:\n",
    "                ng += 1\n",
    "        print(\"正答数 : {}\".format(ok))\n",
    "        print(\"誤答数 : {}\".format(ng))\n",
    "        print(\"正答率 : {}%\".format(float(ok) * 100.0 / float(ok + ng)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedCNN:\n",
    "    def __init__(self, input_dim=(1,28,28), hidden_size=50, output_size=10,\n",
    "                 cparam={'filter_size':30, 'filter_h':5, 'filter_w':5, 'stride':1, 'padding':0}):\n",
    "        conv_output_size = (input_dim[1] - cparam['filter_h'] + 2 * cparam['padding']) / cparam['stride'] + 1\n",
    "        pool_output_size = int(cparam['filter_size'] * (conv_output_size / 2) * (conv_output_size / 2))\n",
    "        # パラメータ初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = he(input_dim[0] * input_dim[1] * input_dim[2], (cparam['filter_size'], input_dim[0], cparam['filter_h'], cparam['filter_w']))\n",
    "        self.params['W2'] = he(pool_output_size, (pool_output_size, hidden_size))\n",
    "        self.params['W3'] = he(hidden_size, (hidden_size, output_size))\n",
    "        self.params['b1'] = np.zeros(cparam['filter_size'])\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        # NN生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], cparam['stride'], cparam['padding'])\n",
    "        self.layers['ReLU'] = ReLU()\n",
    "        self.layers['Pool1'] = Pooling(2, 2, 2, 0)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['ReLU'] = ReLU()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.lastLayer = SoftmaxWithCrossEntropy()\n",
    "        \n",
    "    # 順伝播\n",
    "    def forward(self, x, t, tflag=True):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x, train_flag=tflag)\n",
    "        y, loss = self.lastLayer.forward(x, t)\n",
    "        return y, loss\n",
    "    \n",
    "    # 逆伝播\n",
    "    def backpropagation(self):\n",
    "        dy = 1.0\n",
    "        dy = self.lastLayer.backward(dy)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dy = layer.backward(dy)\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        return grads\n",
    "    \n",
    "    # ファイルへの保存\n",
    "    def save_params(self, filename):\n",
    "        np.savez(filename, W1=self.params['W1'], W2=self.params['W2'], W3=self.params['W3'], \n",
    "                 b1=self.params['b1'], b2=self.params['b2'], b3=self.params['b3'])\n",
    "    \n",
    "    # ファイルからの読み込み\n",
    "    def load_params(self, filename):\n",
    "        npz = np.load(filename)\n",
    "        self.params['W1'] = npz['W1']\n",
    "        self.params['W2'] = npz['W2']\n",
    "        self.params['W3'] = npz['W3']\n",
    "        self.params['b1'] = npz['b1']\n",
    "        self.params['b2'] = npz['b2']\n",
    "        self.params['b3'] = npz['b3']\n",
    "        self.layers['Conv1'].W = self.params['W1']\n",
    "        self.layers['Conv1'].b = self.params['b1']\n",
    "        self.layers['Affine1'].W = self.params['W2']\n",
    "        self.layers['Affine1'].b = self.params['b2']\n",
    "        self.layers['Affine2'].W = self.params['W3']\n",
    "        self.layers['Affine2'].b = self.params['b3']\n",
    "    \n",
    "    # テストデータを用いた精度確認\n",
    "    def test_acc(self, x, t):\n",
    "        ok = 0\n",
    "        ng = 0\n",
    "        for i in range(x.shape[0]):\n",
    "            xx = x[[i]]\n",
    "            tt = t[[i]]\n",
    "            y, e = self.forward(xx, tt, False)\n",
    "            res = np.argmax(y)\n",
    "            ans = np.argmax(tt)\n",
    "            if res == ans:\n",
    "                ok += 1\n",
    "            else:\n",
    "                ng += 1\n",
    "        print(\"正答数 : {}\".format(ok))\n",
    "        print(\"誤答数 : {}\".format(ng))\n",
    "        print(\"正答率 : {}%\".format(float(ok) * 100.0 / float(ok + ng)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチ学習\n",
    "def minibatch(network, optimizer, filename, is_cnn=False):\n",
    "    for i in range(epoch):\n",
    "        e = 0.0\n",
    "        for _ in range(int(n / b)):\n",
    "            # ランダムに画像を選ぶ\n",
    "            index = np.random.choice(n, b, replace=True)\n",
    "            x = X_train[index]\n",
    "            t = T_train[index]\n",
    "            # CNNでないなら入力の形を整える\n",
    "            if not is_cnn:\n",
    "                x = x.reshape((b, d))\n",
    "                # t = t.reshape((b, c))\n",
    "            # 順伝播\n",
    "            y, e = network.forward(x, t)\n",
    "            # 誤差逆伝播法\n",
    "            grads = network.backpropagation()\n",
    "            # パラメータ更新\n",
    "            optimizer.update(network.params, grads)\n",
    "        print(\"{}エポック目の誤差平均 : {}\".format(i + 1, e / (n / b)))\n",
    "    network.save_params(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1エポック目の誤差平均 : 0.003230982294974653\n",
      "2エポック目の誤差平均 : 0.002430484251462343\n",
      "3エポック目の誤差平均 : 0.001944492544372093\n",
      "4エポック目の誤差平均 : 0.0015860593952860592\n",
      "5エポック目の誤差平均 : 0.0013823642708127495\n",
      "6エポック目の誤差平均 : 0.0012487544986569145\n",
      "7エポック目の誤差平均 : 0.0009918239441929243\n",
      "8エポック目の誤差平均 : 0.0010450638999146267\n",
      "9エポック目の誤差平均 : 0.000997694291884218\n",
      "10エポック目の誤差平均 : 0.0008123697800993858\n",
      "11エポック目の誤差平均 : 0.0007987189547510484\n",
      "12エポック目の誤差平均 : 0.0007259514270028853\n",
      "13エポック目の誤差平均 : 0.0009560844296470657\n",
      "14エポック目の誤差平均 : 0.0006584579416929816\n",
      "15エポック目の誤差平均 : 0.0006301444338584097\n",
      "16エポック目の誤差平均 : 0.0006471637735964611\n",
      "17エポック目の誤差平均 : 0.00040680491304802177\n",
      "18エポック目の誤差平均 : 0.00046733768513873065\n",
      "19エポック目の誤差平均 : 0.0005460591719597601\n",
      "20エポック目の誤差平均 : 0.0005025941471894595\n",
      "21エポック目の誤差平均 : 0.000596715013404952\n",
      "22エポック目の誤差平均 : 0.000619088105322979\n",
      "23エポック目の誤差平均 : 0.0005721612999925449\n",
      "24エポック目の誤差平均 : 0.000516929867583436\n",
      "25エポック目の誤差平均 : 0.0005084251954249819\n",
      "26エポック目の誤差平均 : 0.00042112745305897714\n",
      "27エポック目の誤差平均 : 0.0004270590858410093\n",
      "28エポック目の誤差平均 : 0.0007629182310058616\n",
      "29エポック目の誤差平均 : 0.0004014378709309872\n",
      "30エポック目の誤差平均 : 0.0004381844749551627\n",
      "正答数 : 9125\n",
      "誤答数 : 875\n",
      "正答率 : 91.25%\n"
     ]
    }
   ],
   "source": [
    "# 3層NNによる学習の場合（SGD）\n",
    "minibatch(NormalNN(d, m, c), SGD(), 'data/nn_sgd')\n",
    "\n",
    "# 精度確認\n",
    "nn = NormalNN(d, m, c)\n",
    "nn.load_params('data/nn_sgd.npz')\n",
    "nn.test_acc(X_test, T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1エポック目の誤差平均 : 0.0009170589151046058\n",
      "2エポック目の誤差平均 : 0.0004922766698682133\n",
      "3エポック目の誤差平均 : 0.0005069294970901675\n",
      "4エポック目の誤差平均 : 0.0004987108929336217\n",
      "5エポック目の誤差平均 : 0.0003005100915198889\n",
      "6エポック目の誤差平均 : 0.0005848081518004932\n",
      "7エポック目の誤差平均 : 0.00029547253762649684\n",
      "8エポック目の誤差平均 : 0.0005371116658460112\n",
      "9エポック目の誤差平均 : 0.0003707289058919259\n",
      "10エポック目の誤差平均 : 0.00045416597064473134\n",
      "11エポック目の誤差平均 : 0.00033057109533457047\n",
      "12エポック目の誤差平均 : 0.0005201103881766894\n",
      "13エポック目の誤差平均 : 0.0003884829560452636\n",
      "14エポック目の誤差平均 : 0.0003303720501580523\n",
      "15エポック目の誤差平均 : 0.0003320998903524091\n",
      "16エポック目の誤差平均 : 0.0002654708110043512\n",
      "17エポック目の誤差平均 : 0.0002960487663365769\n",
      "18エポック目の誤差平均 : 0.00016340513997812058\n",
      "19エポック目の誤差平均 : 0.00011523825688937343\n",
      "20エポック目の誤差平均 : 0.0002106727818665434\n",
      "21エポック目の誤差平均 : 0.00015044828064279863\n",
      "22エポック目の誤差平均 : 0.0003232454846308064\n",
      "23エポック目の誤差平均 : 0.0003443700165041184\n",
      "24エポック目の誤差平均 : 0.00021194715288531238\n",
      "25エポック目の誤差平均 : 0.0002755369661250295\n",
      "26エポック目の誤差平均 : 0.0002794599850470879\n",
      "27エポック目の誤差平均 : 0.0003239873964497166\n",
      "28エポック目の誤差平均 : 0.00026166151268327073\n",
      "29エポック目の誤差平均 : 7.855898272105384e-05\n",
      "30エポック目の誤差平均 : 0.00015063566649731855\n",
      "正答数 : 9587\n",
      "誤答数 : 413\n",
      "正答率 : 95.87%\n"
     ]
    }
   ],
   "source": [
    "# 3層NNによる学習の場合（MomentumSGD）\n",
    "minibatch(NormalNN(d, m, c), MomentumSGD(), 'data/nn_momentumsgd')\n",
    "\n",
    "# 精度確認\n",
    "nn = NormalNN(d, m, c)\n",
    "nn.load_params('data/nn_momentumsgd.npz')\n",
    "nn.test_acc(X_test, T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1エポック目の誤差平均 : 0.002699746037917266\n",
      "2エポック目の誤差平均 : 0.0024076876628265505\n",
      "3エポック目の誤差平均 : 0.0020437765571028306\n",
      "4エポック目の誤差平均 : 0.0019217827332446355\n",
      "5エポック目の誤差平均 : 0.0018491958924067105\n",
      "6エポック目の誤差平均 : 0.001973900814797447\n",
      "7エポック目の誤差平均 : 0.0017190891306828248\n",
      "8エポック目の誤差平均 : 0.001485221980465757\n",
      "9エポック目の誤差平均 : 0.0015139644443923933\n",
      "10エポック目の誤差平均 : 0.0013623652596518943\n",
      "11エポック目の誤差平均 : 0.001470994912059193\n",
      "12エポック目の誤差平均 : 0.0012590992787248974\n",
      "13エポック目の誤差平均 : 0.0013434231871932033\n",
      "14エポック目の誤差平均 : 0.0012872251924871847\n",
      "15エポック目の誤差平均 : 0.0013145679301937597\n",
      "16エポック目の誤差平均 : 0.0011605573571003478\n",
      "17エポック目の誤差平均 : 0.0011123032784942448\n",
      "18エポック目の誤差平均 : 0.0011482546821854202\n",
      "19エポック目の誤差平均 : 0.0012028306010702541\n",
      "20エポック目の誤差平均 : 0.0010751451753975704\n",
      "21エポック目の誤差平均 : 0.0012194055376808367\n",
      "22エポック目の誤差平均 : 0.0011355469987826795\n",
      "23エポック目の誤差平均 : 0.0009393366866362597\n",
      "24エポック目の誤差平均 : 0.0011071817940525913\n",
      "25エポック目の誤差平均 : 0.001162630875411259\n",
      "26エポック目の誤差平均 : 0.0011392433743602631\n",
      "27エポック目の誤差平均 : 0.0010071370067997916\n",
      "28エポック目の誤差平均 : 0.000946021931725723\n",
      "29エポック目の誤差平均 : 0.0009438527003410784\n",
      "30エポック目の誤差平均 : 0.0009415538605828537\n",
      "正答数 : 8854\n",
      "誤答数 : 1146\n",
      "正答率 : 88.54%\n"
     ]
    }
   ],
   "source": [
    "# 3層NNによる学習の場合（AdaGrad）\n",
    "minibatch(NormalNN(d, m, c), AdaGrad(), 'data/nn_adagrad')\n",
    "\n",
    "# 精度確認\n",
    "nn = NormalNN(d, m, c)\n",
    "nn.load_params('data/nn_adagrad.npz')\n",
    "nn.test_acc(X_test, T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1エポック目の誤差平均 : 0.0006107484850971527\n",
      "2エポック目の誤差平均 : 0.00036151545668083196\n",
      "3エポック目の誤差平均 : 0.0004577269076388583\n",
      "4エポック目の誤差平均 : 0.00022868240412386747\n",
      "5エポック目の誤差平均 : 0.0002901181780800104\n",
      "6エポック目の誤差平均 : 0.00027808058405544305\n",
      "7エポック目の誤差平均 : 0.00021577197356564186\n",
      "8エポック目の誤差平均 : 0.00018086362296312774\n",
      "9エポック目の誤差平均 : 0.00014608684264499734\n",
      "10エポック目の誤差平均 : 0.00012957332354950364\n",
      "11エポック目の誤差平均 : 7.05397362230752e-05\n",
      "12エポック目の誤差平均 : 0.00023551850724913375\n",
      "13エポック目の誤差平均 : 0.0001144605460927488\n",
      "14エポック目の誤差平均 : 9.877202730979158e-05\n",
      "15エポック目の誤差平均 : 6.387800674453211e-05\n",
      "16エポック目の誤差平均 : 3.560283544964944e-05\n",
      "17エポック目の誤差平均 : 7.180016719589282e-05\n",
      "18エポック目の誤差平均 : 9.458409588069973e-05\n",
      "19エポック目の誤差平均 : 0.000116684268094468\n",
      "20エポック目の誤差平均 : 3.6503578344969984e-05\n",
      "21エポック目の誤差平均 : 0.0001123139804153818\n",
      "22エポック目の誤差平均 : 4.446599293003225e-05\n",
      "23エポック目の誤差平均 : 0.0001035514743994188\n",
      "24エポック目の誤差平均 : 4.674150134740644e-05\n",
      "25エポック目の誤差平均 : 4.543069337253908e-05\n",
      "26エポック目の誤差平均 : 7.34526214778222e-05\n",
      "27エポック目の誤差平均 : 4.355560840658794e-05\n",
      "28エポック目の誤差平均 : 1.639383931882135e-05\n",
      "29エポック目の誤差平均 : 6.387016698786227e-05\n",
      "30エポック目の誤差平均 : 5.532852823690678e-05\n",
      "正答数 : 9703\n",
      "誤答数 : 297\n",
      "正答率 : 97.03%\n"
     ]
    }
   ],
   "source": [
    "# 3層NNによる学習の場合（RMSProp）\n",
    "minibatch(NormalNN(d, m, c), RMSProp(), 'data/nn_rmsprop')\n",
    "\n",
    "# 精度確認\n",
    "nn = NormalNN(d, m, c)\n",
    "nn.load_params('data/nn_rmsprop.npz')\n",
    "nn.test_acc(X_test, T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1エポック目の誤差平均 : 0.0003696555696290338\n",
      "2エポック目の誤差平均 : 0.0002674554819990995\n",
      "3エポック目の誤差平均 : 0.00044525392508610977\n",
      "4エポック目の誤差平均 : 8.871750456502476e-05\n",
      "5エポック目の誤差平均 : 0.00016565361945473936\n",
      "6エポック目の誤差平均 : 0.00036773114363902216\n",
      "7エポック目の誤差平均 : 0.00022616711815137274\n",
      "8エポック目の誤差平均 : 0.0002840510843193345\n",
      "9エポック目の誤差平均 : 0.0001294997390013683\n",
      "10エポック目の誤差平均 : 4.333845725743318e-05\n",
      "11エポック目の誤差平均 : 8.284190287762127e-05\n",
      "12エポック目の誤差平均 : 5.6324458865087554e-05\n",
      "13エポック目の誤差平均 : 7.242155894988428e-05\n",
      "14エポック目の誤差平均 : 0.00010365674217056174\n",
      "15エポック目の誤差平均 : 0.00024118980228985458\n",
      "16エポック目の誤差平均 : 8.452890040396287e-05\n",
      "17エポック目の誤差平均 : 7.446230695829858e-05\n",
      "18エポック目の誤差平均 : 8.439469706186386e-05\n",
      "19エポック目の誤差平均 : 4.932371849083162e-05\n",
      "20エポック目の誤差平均 : 0.00013916332196908985\n",
      "21エポック目の誤差平均 : 3.7198701304758195e-05\n",
      "22エポック目の誤差平均 : 6.73820220184961e-05\n",
      "23エポック目の誤差平均 : 0.00011926777077621603\n",
      "24エポック目の誤差平均 : 5.201130422788114e-05\n",
      "25エポック目の誤差平均 : 5.1158630416826354e-05\n",
      "26エポック目の誤差平均 : 4.330485882611456e-05\n",
      "27エポック目の誤差平均 : 2.4029761961964918e-05\n",
      "28エポック目の誤差平均 : 3.228773915215381e-05\n",
      "29エポック目の誤差平均 : 4.763638445808223e-05\n",
      "30エポック目の誤差平均 : 8.764802626915253e-05\n",
      "正答数 : 9723\n",
      "誤答数 : 277\n",
      "正答率 : 97.23%\n"
     ]
    }
   ],
   "source": [
    "# 3層NNによる学習の場合（AdaDelta）\n",
    "minibatch(NormalNN(d, m, c), AdaDelta(), 'data/nn_adadelta')\n",
    "\n",
    "# 精度確認\n",
    "nn = NormalNN(d, m, c)\n",
    "nn.load_params('data/nn_adadelta.npz')\n",
    "nn.test_acc(X_test, T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1エポック目の誤差平均 : 0.0007296712235649719\n",
      "2エポック目の誤差平均 : 0.0002571260343618341\n",
      "3エポック目の誤差平均 : 0.00041331072209553556\n",
      "4エポック目の誤差平均 : 0.00037656193624252055\n",
      "5エポック目の誤差平均 : 0.00042374682931320304\n",
      "6エポック目の誤差平均 : 0.00020112748163642524\n",
      "7エポック目の誤差平均 : 0.0002534457770944605\n",
      "8エポック目の誤差平均 : 0.0002627248285893633\n",
      "9エポック目の誤差平均 : 0.00012990757665279194\n",
      "10エポック目の誤差平均 : 0.00018507210281015939\n",
      "11エポック目の誤差平均 : 7.67619165076512e-05\n",
      "12エポック目の誤差平均 : 8.71444621943942e-05\n",
      "13エポック目の誤差平均 : 0.00035254946329446704\n",
      "14エポック目の誤差平均 : 0.00011337848661489618\n",
      "15エポック目の誤差平均 : 6.00244508718523e-05\n",
      "16エポック目の誤差平均 : 5.988085234177581e-05\n",
      "17エポック目の誤差平均 : 9.15693720341259e-05\n",
      "18エポック目の誤差平均 : 0.00017976875142821432\n",
      "19エポック目の誤差平均 : 5.051794565212939e-05\n",
      "20エポック目の誤差平均 : 5.9931559706469966e-05\n",
      "21エポック目の誤差平均 : 8.532454319590736e-05\n",
      "22エポック目の誤差平均 : 0.0001235266477946897\n",
      "23エポック目の誤差平均 : 8.145522999191287e-05\n",
      "24エポック目の誤差平均 : 2.7634597137849618e-05\n",
      "25エポック目の誤差平均 : 2.511013902557498e-05\n",
      "26エポック目の誤差平均 : 3.2276286336446836e-05\n",
      "27エポック目の誤差平均 : 8.868079927479484e-05\n",
      "28エポック目の誤差平均 : 6.096397008944928e-05\n",
      "29エポック目の誤差平均 : 1.7008238256786293e-05\n",
      "30エポック目の誤差平均 : 7.484475074341844e-05\n",
      "正答数 : 9709\n",
      "誤答数 : 291\n",
      "正答率 : 97.09%\n"
     ]
    }
   ],
   "source": [
    "# 3層NNによる学習の場合（Adam）\n",
    "minibatch(NormalNN(d, m, c), Adam(), 'data/nn_adam')\n",
    "\n",
    "# 精度確認\n",
    "nn = NormalNN(d, m, c)\n",
    "nn.load_params('data/nn_adam.npz')\n",
    "nn.test_acc(X_test, T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1エポック目の誤差平均 : 0.0008005042421458591\n",
      "2エポック目の誤差平均 : 0.0006284837875793833\n",
      "3エポック目の誤差平均 : 0.0003554889432052642\n",
      "4エポック目の誤差平均 : 0.00021164253999073174\n",
      "5エポック目の誤差平均 : 0.00015563673224064584\n",
      "6エポック目の誤差平均 : 0.00029648501331076805\n",
      "7エポック目の誤差平均 : 0.0001925971492468666\n",
      "8エポック目の誤差平均 : 0.00026221754639713644\n",
      "9エポック目の誤差平均 : 0.00032117175876731583\n",
      "10エポック目の誤差平均 : 7.651652302728934e-05\n",
      "11エポック目の誤差平均 : 0.00010888380823125426\n",
      "12エポック目の誤差平均 : 7.768248568942125e-05\n",
      "13エポック目の誤差平均 : 0.00020810921180754906\n",
      "14エポック目の誤差平均 : 7.826707649080612e-05\n",
      "15エポック目の誤差平均 : 0.00016151644129294365\n",
      "16エポック目の誤差平均 : 6.60239512785564e-05\n",
      "17エポック目の誤差平均 : 6.656579415848662e-05\n",
      "18エポック目の誤差平均 : 5.785200967487481e-05\n",
      "19エポック目の誤差平均 : 0.00010915171241448725\n",
      "20エポック目の誤差平均 : 6.373462541877408e-05\n",
      "21エポック目の誤差平均 : 7.058187067439193e-05\n",
      "22エポック目の誤差平均 : 8.11965595003555e-05\n",
      "23エポック目の誤差平均 : 0.00014538530797395148\n",
      "24エポック目の誤差平均 : 0.00020123535885998423\n",
      "25エポック目の誤差平均 : 7.80914719389644e-05\n",
      "26エポック目の誤差平均 : 7.278606909204043e-05\n",
      "27エポック目の誤差平均 : 5.837302546900465e-05\n",
      "28エポック目の誤差平均 : 7.381921414112754e-05\n",
      "29エポック目の誤差平均 : 3.29605966737163e-05\n",
      "30エポック目の誤差平均 : 7.320386663647523e-05\n",
      "正答数 : 9194\n",
      "誤答数 : 806\n",
      "正答率 : 91.94%\n"
     ]
    }
   ],
   "source": [
    "# 3層NNによる学習の場合（ReLU + BatchNormalization + AdaDelta）\n",
    "minibatch(AdvancedNN(d, m, c), AdaDelta(), 'data/ann_bn_adadelta')\n",
    "\n",
    "# 精度確認\n",
    "nn = AdvancedNN(d, m, c)\n",
    "nn.load_params('data/ann_bn_adadelta.npz')\n",
    "nn.test_acc(X_test, T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正答数 : 9865\n",
      "誤答数 : 135\n",
      "正答率 : 98.65%\n"
     ]
    }
   ],
   "source": [
    "# CNNによる学習の場合\n",
    "# かなり時間がかかる\n",
    "# minibatch(AdvancedCNN(), RMSProp(), 'data/advanced_cnn', True)\n",
    "\n",
    "# 精度確認\n",
    "cnn = AdvancedCNN()\n",
    "cnn.load_params('data/advanced_cnn.npz')\n",
    "cnn.test_acc(X_test, T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
